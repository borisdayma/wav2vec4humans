program: run_common_voice.py
# if using a common dashboard, you can use wandb ; if unspecified, it's your own workspace
entity: wandb
# if using wandb dashboards, check created dashboards such as xlsr-french
project: xlsr
# right now I do a random search but if you're in optimizing phase, you could try "bayes"
method: random
metric:
  name: test/wer
  goal: minimize
parameters:
  attention_dropout:
    distribution: log_uniform
    # from 0.1/2 to 0.1*2 - values provided are ln(min) -> ln(max)
    min: -3.4
    max: -1.2
  activation_dropout:
    distribution: log_uniform
    min: -3.4
    max: -1.2
  hidden_dropout:
    distribution: log_uniform
    min: -3.4
    max: -1.2
  feat_proj_dropout:
    distribution: log_uniform
    min: -3.4
    max: -1.2
  mask_time_prob:
    distribution: log_uniform
    # from 0.05/2 to 0.05*2 - values provided are ln(min) -> ln(max)
    min: -4.1
    max: -1.9
  layerdrop:
    distribution: log_uniform
    # from 0.05/2 to 0.05*2 - values provided are ln(min) -> ln(max)
    min: -3.4
    max: -1.2
  learning_rate:
    distribution: log_uniform
    min: -9.2
    max: -6.9
  gradient_accumulation_steps:
    distribution: int_uniform
    min: 1
    max: 4
command:
  - python
  - ${program}
  - "--model_name_or_path"
  - "facebook/wav2vec2-large-xlsr-53"
  - "--dataset_config_name"
  - "tr"
  - "--output_dir"
  - "./model"
  - "--overwrite_output_dir"
  - "--num_train_epochs"
  - 10
  - "--per_device_train_batch_size"
  - 16
  - "--evaluation_strategy"
  - "epoch"
  - "--fp16"
  - "--freeze_feature_extractor"
  - "--group_by_length"
  - "--gradient_checkpointing"
  - "--do_train"
  - "--save_total_limit"
  - 1
  - "--logging_steps"
  - 100
  - "--warmup_steps"
  - 500
  - "--load_best_model_at_end"
  - "--metric_for_best_model"
  - "wer"
  - "--greater_is_better"
  - "False"
  - ${args}